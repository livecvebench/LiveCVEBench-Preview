"""
Vulnerability tests for brotli decompression bomb in bugsink.

These tests verify that the brotli decompression is properly bounded to prevent
memory exhaustion attacks. A small compressed payload of zeros can expand to
gigabytes if not properly limited.

In VULNERABLE state: Tests should FAIL (vulnerability is exploitable)
In FIXED state: Tests should PASS (memory usage is bounded)

The fix uses output_buffer_limit parameter in brotli.Decompressor.process()
to ensure each decompression call produces bounded output.
"""

import brotli
import io
import sys
import os
import tracemalloc
import gc
import importlib.util

# Mock the Django and bugsink dependencies BEFORE importing bugsink
class MockSettings:
    def __getitem__(self, key):
        return 10 * 1024 * 1024  # 10MB default

class BadRequest(Exception):
    pass

# Function to do nothing for assert_
def mock_assert(x, msg=None):
    pass

# We need to import bugsink.streams directly without triggering bugsink/__init__.py
# This is done by loading the module file directly using importlib

# First, set up the mock modules that streams.py imports
mock_django = type(sys)('django')
mock_django_core = type(sys)('django.core')
mock_django_core_exceptions = type(sys)('django.core.exceptions')
mock_django_core_exceptions.BadRequest = BadRequest

sys.modules['django'] = mock_django
sys.modules['django.core'] = mock_django_core
sys.modules['django.core.exceptions'] = mock_django_core_exceptions

mock_app_settings = type(sys)('bugsink.app_settings')
mock_app_settings.get_settings = lambda: MockSettings()
sys.modules['bugsink.app_settings'] = mock_app_settings

mock_utils = type(sys)('bugsink.utils')
mock_utils.assert_ = mock_assert
sys.modules['bugsink.utils'] = mock_utils

# Now load streams.py directly using importlib, bypassing bugsink/__init__.py
spec = importlib.util.spec_from_file_location("bugsink.streams", "/app/bugsink/streams.py")
streams_module = importlib.util.module_from_spec(spec)
sys.modules['bugsink.streams'] = streams_module
spec.loader.exec_module(streams_module)

# Extract what we need from the module
brotli_generator = streams_module.brotli_generator
GeneratorReader = streams_module.GeneratorReader
DEFAULT_CHUNK_SIZE = streams_module.DEFAULT_CHUNK_SIZE


def create_brotli_bomb(size_mb):
    """
    Create a brotli bomb that decompresses to size_mb megabytes of zeros.
    Zeros compress extremely well with brotli, creating a "bomb" effect.
    """
    data = b'\x00' * (size_mb * 1024 * 1024)
    return brotli.compress(data, quality=11)


class TestBrotliBombProtection:
    """
    Test that brotli decompression protects against decompression bombs.

    The vulnerability occurs when a small compressed payload expands to
    a huge amount of data in memory. The fix ensures each decompression
    call produces bounded output using output_buffer_limit.
    """

    def test_15mb_bomb_memory_bounded(self):
        """
        Test that a 15MB brotli bomb doesn't allocate excessive memory per chunk.

        In the vulnerable version, decompressing a 15MB bomb will allocate
        ~15MB in a single decompressor.process() call. In the fixed version,
        each call is bounded by output_buffer_limit (DEFAULT_CHUNK_SIZE = 8KB).
        """
        uncompressed_size = 15 * 1024 * 1024  # 15MB
        bomb = create_brotli_bomb(15)

        # Force garbage collection to get accurate memory measurement
        gc.collect()
        tracemalloc.start()

        stream = io.BytesIO(bomb)
        gen = brotli_generator(stream)

        # Track the maximum single chunk size yielded by the generator
        max_chunk_size = 0
        chunks = []

        for chunk in gen:
            if len(chunk) > max_chunk_size:
                max_chunk_size = len(chunk)
            chunks.append(chunk)

        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        result = b"".join(chunks)

        # Verify decompression worked correctly
        assert len(result) == uncompressed_size, \
            f"Expected {uncompressed_size} bytes, got {len(result)}"

        # In the fixed version, chunks should be bounded by chunk_size
        # The actual output may be larger than output_buffer_limit due to
        # implementation details, but should not be unboundedly larger.
        # We use a generous threshold: chunks should be < 256KB
        # (compared to 15MB in the vulnerable version)
        threshold = 256 * 1024  # 256KB threshold

        assert max_chunk_size < threshold, \
            f"Max chunk size {max_chunk_size} exceeds threshold {threshold}. " \
            f"In vulnerable code, chunks can be as large as the entire " \
            f"uncompressed data ({uncompressed_size} bytes)."

    def test_5mb_bomb_chunk_bounded(self):
        """
        Test 5MB bomb with explicit chunk size verification.
        """
        uncompressed_size = 5 * 1024 * 1024  # 5MB
        bomb = create_brotli_bomb(5)

        stream = io.BytesIO(bomb)
        gen = brotli_generator(stream)

        max_chunk_size = 0
        total_size = 0

        for chunk in gen:
            if len(chunk) > max_chunk_size:
                max_chunk_size = len(chunk)
            total_size += len(chunk)

        assert total_size == uncompressed_size

        # Chunks should be reasonably bounded
        threshold = 256 * 1024  # 256KB
        assert max_chunk_size < threshold, \
            f"Chunk size {max_chunk_size} exceeds {threshold}. " \
            f"The generator should yield bounded chunks, not entire payloads."

    def test_10mb_bomb_multiple_chunks(self):
        """
        Test that a 10MB bomb produces multiple bounded chunks.

        In the vulnerable version, this would produce 1-2 huge chunks.
        In the fixed version, it should produce many smaller chunks.
        """
        uncompressed_size = 10 * 1024 * 1024  # 10MB
        bomb = create_brotli_bomb(10)

        stream = io.BytesIO(bomb)
        gen = brotli_generator(stream)

        chunk_count = 0
        chunk_sizes = []

        for chunk in gen:
            chunk_count += 1
            chunk_sizes.append(len(chunk))

        total = sum(chunk_sizes)
        assert total == uncompressed_size

        # In the fixed version, we should have many chunks because output
        # is bounded. The vulnerable version may produce just 1-2 huge chunks.
        # With 8KB chunk_size and 10MB data, we expect at least 100 chunks.
        # Being generous with threshold: at least 10 chunks.
        min_expected_chunks = 10

        assert chunk_count >= min_expected_chunks, \
            f"Only {chunk_count} chunks produced. Expected at least {min_expected_chunks}. " \
            f"The fixed generator should produce many bounded chunks instead of few large ones."

    def test_varying_bomb_sizes(self):
        """
        Test various bomb sizes to ensure consistent bounded behavior.
        """
        sizes_mb = [1, 3, 8, 12]

        for size_mb in sizes_mb:
            uncompressed_size = size_mb * 1024 * 1024
            bomb = create_brotli_bomb(size_mb)

            stream = io.BytesIO(bomb)
            gen = brotli_generator(stream)

            max_chunk = 0
            total = 0

            for chunk in gen:
                if len(chunk) > max_chunk:
                    max_chunk = len(chunk)
                total += len(chunk)

            assert total == uncompressed_size, \
                f"Size mismatch for {size_mb}MB bomb"

            threshold = 256 * 1024
            assert max_chunk < threshold, \
                f"For {size_mb}MB bomb: max chunk {max_chunk} exceeds {threshold}"


class TestMemoryExhaustionPrevention:
    """
    Test that memory usage is bounded during decompression.
    """

    def test_peak_memory_bounded(self):
        """
        Test that peak memory during decompression is bounded.

        In the vulnerable version, peak memory will be close to the
        uncompressed size (15MB). In the fixed version, memory should
        be bounded because chunks are processed incrementally.
        """
        # Create bomb BEFORE starting tracemalloc to avoid measuring
        # the memory used by test data creation
        bomb = create_brotli_bomb(15)  # 15MB when decompressed

        gc.collect()
        tracemalloc.start()

        stream = io.BytesIO(bomb)
        reader = GeneratorReader(brotli_generator(stream))

        # Read in chunks to simulate streaming processing
        chunk_size = 64 * 1024  # 64KB read chunks
        total = 0

        while True:
            chunk = reader.read(chunk_size)
            if not chunk:
                break
            total += len(chunk)

        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        expected_total = 15 * 1024 * 1024
        assert total == expected_total

        # In fixed version with proper streaming, peak memory should be
        # much less than the total uncompressed size during iteration.
        # We check that peak memory is less than 50% of total data size.
        peak_mb = peak / (1024 * 1024)
        print(f"Peak memory during streaming: {peak_mb:.2f} MB")

        # Add assertion: peak memory should be less than 8MB for streaming
        # In vulnerable code, it will be ~15MB+
        assert peak < 8 * 1024 * 1024, \
            f"Peak memory {peak_mb:.2f} MB exceeds 8MB threshold. " \
            f"Vulnerable code allocates all data at once."

    def test_repeated_processing_no_memory_leak(self):
        """
        Test that processing multiple bombs has bounded chunk sizes.
        This tests that each bomb produces bounded chunks, not unbounded ones.
        """
        bomb = create_brotli_bomb(2)  # 2MB bomb

        max_chunk_across_iterations = 0

        for i in range(5):
            stream = io.BytesIO(bomb)
            gen = brotli_generator(stream)

            for chunk in gen:
                if len(chunk) > max_chunk_across_iterations:
                    max_chunk_across_iterations = len(chunk)

        # All chunks should be bounded even across multiple iterations
        threshold = 256 * 1024  # 256KB
        assert max_chunk_across_iterations < threshold, \
            f"Max chunk size {max_chunk_across_iterations} exceeds {threshold} across iterations"


class TestOutputBufferLimit:
    """
    Test the output_buffer_limit parameter behavior.

    In Brotli >= 1.2.0, process() accepts output_buffer_limit to cap output.
    The fix uses this to prevent unbounded memory allocation.
    """

    def test_chunk_size_respected(self):
        """
        Test that output respects chunk_size boundaries.
        """
        # Create a 3MB bomb
        uncompressed_size = 3 * 1024 * 1024
        bomb = create_brotli_bomb(3)

        stream = io.BytesIO(bomb)
        gen = brotli_generator(stream, chunk_size=DEFAULT_CHUNK_SIZE)

        chunks = list(gen)

        # All chunks except possibly the last should be bounded
        # Due to brotli implementation details, actual chunks may be
        # larger than output_buffer_limit but should still be bounded
        max_chunk = max(len(c) for c in chunks)

        # Very generous threshold: 512KB (64x the 8KB chunk_size)
        # This allows for implementation variance while still detecting
        # the vulnerable unbounded behavior
        assert max_chunk < 512 * 1024, \
            f"Max chunk {max_chunk} is too large, suggesting unbounded decompression"

    def test_custom_chunk_size(self):
        """
        Test with custom chunk size parameter.
        """
        # Use a smaller chunk size
        custom_chunk = 4 * 1024  # 4KB

        bomb = create_brotli_bomb(2)  # 2MB

        stream = io.BytesIO(bomb)
        gen = brotli_generator(stream, chunk_size=custom_chunk)

        chunks = list(gen)
        total = sum(len(c) for c in chunks)

        assert total == 2 * 1024 * 1024

        # Should produce many chunks with smaller chunk_size
        assert len(chunks) > 1, "Should produce multiple chunks"


class TestAttackVectors:
    """
    Test various attack vector patterns.
    """

    def test_all_zeros_bomb(self):
        """
        Test bomb made of all zeros (maximum compression ratio).
        """
        data = b'\x00' * (8 * 1024 * 1024)  # 8MB of zeros
        compressed = brotli.compress(data, quality=11)

        # Zeros compress extremely well
        compression_ratio = len(data) / len(compressed)
        print(f"Compression ratio for zeros: {compression_ratio:.0f}:1")

        stream = io.BytesIO(compressed)
        gen = brotli_generator(stream)

        max_chunk = 0
        for chunk in gen:
            if len(chunk) > max_chunk:
                max_chunk = len(chunk)

        assert max_chunk < 256 * 1024, \
            f"Max chunk {max_chunk} too large for zeros bomb"

    def test_alternating_pattern_bomb(self):
        """
        Test bomb with alternating byte pattern.
        """
        pattern = b'\x00\xff' * (4 * 1024 * 1024)  # 8MB alternating
        compressed = brotli.compress(pattern, quality=11)

        stream = io.BytesIO(compressed)
        gen = brotli_generator(stream)

        max_chunk = 0
        total = 0
        for chunk in gen:
            if len(chunk) > max_chunk:
                max_chunk = len(chunk)
            total += len(chunk)

        assert total == len(pattern)

        # Chunks should be bounded - alternating pattern should still produce
        # bounded chunks in fixed version
        threshold = 256 * 1024  # 256KB
        assert max_chunk < threshold, \
            f"Max chunk {max_chunk} exceeds {threshold} for alternating pattern bomb"

    def test_repeated_string_bomb(self):
        """
        Test bomb with repeated string (like 'A' * N).
        """
        data = b'A' * (6 * 1024 * 1024)  # 6MB of 'A'
        compressed = brotli.compress(data, quality=11)

        stream = io.BytesIO(compressed)
        gen = brotli_generator(stream)

        max_chunk = 0
        total = 0

        for chunk in gen:
            if len(chunk) > max_chunk:
                max_chunk = len(chunk)
            total += len(chunk)

        assert total == len(data)
        assert max_chunk < 256 * 1024

    def test_maximum_compression_quality(self):
        """
        Test with maximum compression quality (worst case for bombs).
        """
        data = b'\x00' * (5 * 1024 * 1024)  # 5MB

        # Quality 11 gives best compression, worst for bombs
        compressed = brotli.compress(data, quality=11)

        stream = io.BytesIO(compressed)
        gen = brotli_generator(stream)

        max_chunk = max(len(c) for c in gen)

        assert max_chunk < 256 * 1024, \
            "Even with max compression quality, chunks should be bounded"
